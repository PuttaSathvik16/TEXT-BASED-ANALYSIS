{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "902eb72c-51e6-4d68-8394-6f3946f90931",
   "metadata": {},
   "source": [
    "## Classwork - 7 5/11/24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8612378f-70ae-4894-83b5-0c9ebfd2b435",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013802b6-455e-4f1a-adac-cdc97e7b71a3",
   "metadata": {},
   "source": [
    "## Open the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "46e9a00c-4104-4e2d-b954-c5f50b20ade2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>year</th>\n",
       "      <th>country</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speaker</th>\n",
       "      <th>position</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>ALB</td>\n",
       "      <td>Albania</td>\n",
       "      <td>Mr. NAS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33: May I first convey to our President the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>ARG</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Mr. DE PABLO PARDO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>177.\\t : It is a fortunate coincidence that pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>AUS</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Mr. McMAHON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.\\t  It is a pleasure for me to extend to y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>AUT</td>\n",
       "      <td>Austria</td>\n",
       "      <td>Mr. KIRCHSCHLAEGER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>155.\\t  May I begin by expressing to Ambassado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>BEL</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>Mr. HARMEL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>176. No doubt each of us, before coming up to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>BLR</td>\n",
       "      <td>Belarus</td>\n",
       "      <td>Mr. GURINOVICH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n71.\\t. We are today mourning the untimely de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>BOL</td>\n",
       "      <td>Bolivia, Plurinational State of</td>\n",
       "      <td>Mr. CAMACHO OMISTE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135.\\t  I wish to congratulate the President o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>BRA</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Mr. GIBSON BARBOZA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.\\tMr. President, I should like, first of all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>CAN</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Mr. SHARP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nThe General Assembly is fortunate indeed to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>CMR</td>\n",
       "      <td>Cameroon</td>\n",
       "      <td>Mr. AHIDJO</td>\n",
       "      <td>President</td>\n",
       "      <td>: A year ago I came here as the Acting Preside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>COG</td>\n",
       "      <td>Congo</td>\n",
       "      <td>Mr. ICKONGA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122.\\t  I cannot begin my intervention without...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>COL</td>\n",
       "      <td>Colombia</td>\n",
       "      <td>Mr. VASQUEZ CARRIZOSA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mr. President, this visit to the United Nation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>CRI</td>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>Mr. FACIO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.\\t  Mr. President, your election to the Pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>CUB</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>Mr. ALARCON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.\\t  Mr. President, I should first like to co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>DOM</td>\n",
       "      <td>Dominican Republic</td>\n",
       "      <td>Mr FERNANDEZ G.</td>\n",
       "      <td></td>\n",
       "      <td>\\n\\n\\n Mr. President, it was a source of great...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>DZA</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>Mr. YAZID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.  The delegation of Algeria is very pleased ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>ECU</td>\n",
       "      <td>Ecuador</td>\n",
       "      <td>Mr. Benites</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.  It had been my hope that a loftier person...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>FRA</td>\n",
       "      <td>France</td>\n",
       "      <td>Mr. SCHUMANN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.\\t  Within one month, when we celebrate the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>GBR</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Sir Alec DOUGLASHOME</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110.\\t Mr. President, I should like first to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>GHA</td>\n",
       "      <td>Ghana</td>\n",
       "      <td>Mr. OWUSU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>121.\\t I should like to begin by congratulatin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    session  year country                     country_name  \\\n",
       "0        25  1970     ALB                          Albania   \n",
       "1        25  1970     ARG                        Argentina   \n",
       "2        25  1970     AUS                        Australia   \n",
       "3        25  1970     AUT                          Austria   \n",
       "4        25  1970     BEL                          Belgium   \n",
       "5        25  1970     BLR                          Belarus   \n",
       "6        25  1970     BOL  Bolivia, Plurinational State of   \n",
       "7        25  1970     BRA                           Brazil   \n",
       "8        25  1970     CAN                           Canada   \n",
       "9        25  1970     CMR                         Cameroon   \n",
       "10       25  1970     COG                            Congo   \n",
       "11       25  1970     COL                         Colombia   \n",
       "12       25  1970     CRI                       Costa Rica   \n",
       "13       25  1970     CUB                             Cuba   \n",
       "14       25  1970     DOM               Dominican Republic   \n",
       "15       25  1970     DZA                          Algeria   \n",
       "16       25  1970     ECU                          Ecuador   \n",
       "17       25  1970     FRA                           France   \n",
       "18       25  1970     GBR                   United Kingdom   \n",
       "19       25  1970     GHA                            Ghana   \n",
       "\n",
       "                  speaker    position  \\\n",
       "0                 Mr. NAS         NaN   \n",
       "1      Mr. DE PABLO PARDO         NaN   \n",
       "2             Mr. McMAHON         NaN   \n",
       "3      Mr. KIRCHSCHLAEGER         NaN   \n",
       "4              Mr. HARMEL         NaN   \n",
       "5          Mr. GURINOVICH         NaN   \n",
       "6      Mr. CAMACHO OMISTE         NaN   \n",
       "7      Mr. GIBSON BARBOZA         NaN   \n",
       "8               Mr. SHARP         NaN   \n",
       "9              Mr. AHIDJO  President    \n",
       "10            Mr. ICKONGA         NaN   \n",
       "11  Mr. VASQUEZ CARRIZOSA         NaN   \n",
       "12              Mr. FACIO         NaN   \n",
       "13            Mr. ALARCON         NaN   \n",
       "14        Mr FERNANDEZ G.               \n",
       "15              Mr. YAZID         NaN   \n",
       "16            Mr. Benites         NaN   \n",
       "17           Mr. SCHUMANN         NaN   \n",
       "18   Sir Alec DOUGLASHOME         NaN   \n",
       "19              Mr. OWUSU         NaN   \n",
       "\n",
       "                                                 text  \n",
       "0   33: May I first convey to our President the co...  \n",
       "1   177.\\t : It is a fortunate coincidence that pr...  \n",
       "2   100.\\t  It is a pleasure for me to extend to y...  \n",
       "3   155.\\t  May I begin by expressing to Ambassado...  \n",
       "4   176. No doubt each of us, before coming up to ...  \n",
       "5   \\n71.\\t. We are today mourning the untimely de...  \n",
       "6   135.\\t  I wish to congratulate the President o...  \n",
       "7   1.\\tMr. President, I should like, first of all...  \n",
       "8   \\nThe General Assembly is fortunate indeed to ...  \n",
       "9   : A year ago I came here as the Acting Preside...  \n",
       "10  122.\\t  I cannot begin my intervention without...  \n",
       "11  Mr. President, this visit to the United Nation...  \n",
       "12  62.\\t  Mr. President, your election to the Pre...  \n",
       "13  1.\\t  Mr. President, I should first like to co...  \n",
       "14  \\n\\n\\n Mr. President, it was a source of great...  \n",
       "15  1.  The delegation of Algeria is very pleased ...  \n",
       "16  71.  It had been my hope that a loftier person...  \n",
       "17  84.\\t  Within one month, when we celebrate the...  \n",
       "18  110.\\t Mr. President, I should like first to s...  \n",
       "19  121.\\t I should like to begin by congratulatin...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file = \"/Users/puttasathvik/Downloads/TextBasedAnalysis/ClassWork/un-general-debates-blueprint.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df.head(20) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a42539-ff5f-4bb4-8d3e-05aed587589c",
   "metadata": {},
   "source": [
    "## Look at a couple of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b8055fd-4a4f-40ec-8b6a-28bd608ae34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\ufeffIt is indeed a pleasure for me and the members of my delegation to extend to Ambassador Garba our sincere congratulations on his election to the presidency of the forty-fourth session of the General Assembly. His election to this high office is a well-deserved tribute to his personal qualities and experience. I am fully confident that under his able and wise leadership the Assembly will further c'\n",
      "'\\ufeffI wish to join\\nother representatives in congratulating you, Sir, on\\nyour unanimous election as President of the fifty-sixth\\nsession of the General Assembly. We are confident that\\n27\\n\\nunder your able guidance the work of this General\\nAssembly session will be another milestone on the new\\ninternational scene, particularly in confronting the new\\nchallenges facing our world, especially after the\\nextre'\n"
     ]
    }
   ],
   "source": [
    "print(repr(df.iloc[2666][\"text\"][0:400]))\n",
    "print(repr(df.iloc[4726][\"text\"][0:400]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bac104-6418-4840-a876-9049a87dd9b6",
   "metadata": {},
   "source": [
    "## We will split speech into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "547a5c5d-874d-4baf-b3fd-72b81a584311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "df[\"paragraphs\"] = df[\"text\"].map(lambda text: re.split('\\.\\s*\\n', text))\n",
    "df[\"number_of_paragraphs\"] = df[\"paragraphs\"].map(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2271b225-b47e-4de0-8961-104391cf29ca",
   "metadata": {},
   "source": [
    "## Start by getting TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0771a8ee-6571-4a33-8633-734bc73a2719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.11/site-packages (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /opt/anaconda3/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "057a58e9-08bd-4567-ab19-24b7e45361fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7507, 24611)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "\n",
    "# Convert stop words set to a list\n",
    "stopwords_list = list(stopwords)\n",
    "\n",
    "# Initialize TfidfVectorizer with the list of stop words\n",
    "tfidf_text_vectorizer = TfidfVectorizer(stop_words=stopwords_list, min_df=5, max_df=0.7)\n",
    "\n",
    "# Fit and transform the text data\n",
    "tfidf_text_vectors = tfidf_text_vectorizer.fit_transform(df['text'])\n",
    "tfidf_text_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d626ae8-a306-4054-bd3f-5f28e7e1636d",
   "metadata": {},
   "source": [
    "## Make a data frame of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "85dd447b-3c8d-41b1-a90a-3b6de9d4fa64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33: May I first convey to our President the co...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35.\\tThe utilization of the United Nations to ...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.\\tThe whole of progressive mankind recalls ...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.\\tAll this has had well known consequences ...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38.\\tOne of the undeniable proofs that the Uni...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>39.\\tUndoubtedly, such a state of affairs in t...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40.\\tThe liberation movement at the world leve...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>41.\\tPanic-stricken at the impetuous growth of...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>42.\\tAlthough split by numerous contradictions...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>43.\\tIn that connexion we can cite, simultaneo...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  year\n",
       "0  33: May I first convey to our President the co...  1970\n",
       "1  35.\\tThe utilization of the United Nations to ...  1970\n",
       "2  36.\\tThe whole of progressive mankind recalls ...  1970\n",
       "3  37.\\tAll this has had well known consequences ...  1970\n",
       "4  38.\\tOne of the undeniable proofs that the Uni...  1970\n",
       "5  39.\\tUndoubtedly, such a state of affairs in t...  1970\n",
       "6  40.\\tThe liberation movement at the world leve...  1970\n",
       "7  41.\\tPanic-stricken at the impetuous growth of...  1970\n",
       "8  42.\\tAlthough split by numerous contradictions...  1970\n",
       "9  43.\\tIn that connexion we can cite, simultaneo...  1970"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten the paragraphs keeping the years\n",
    "paragraph_df = pd.DataFrame([{ \"text\": paragraph, \"year\": year }\n",
    "for paragraphs, year in zip(df[\"paragraphs\"], df[\"year\"])\n",
    "for paragraph in paragraphs if paragraph])\n",
    "paragraph_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238f4b62-3a28-492a-b4d1-59df05a69cec",
   "metadata": {},
   "source": [
    "## Get the TFIDF of the sentences DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a0e53813-7def-4613-a7ec-7a139af72ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(279076, 25156)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "\n",
    "# Use 'english' instead of the set of stop words\n",
    "tfidf_para_vectorizer = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.7)\n",
    "tfidf_para_vectors = tfidf_para_vectorizer.fit_transform(paragraph_df[\"text\"])\n",
    "tfidf_para_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90ee99-692e-4cca-93fb-2f130c6f1eb9",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f33703c6-5dae-4039-a70c-5d39f0c95fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1665: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf_text_model = NMF(n_components=10, random_state=42)\n",
    "W_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)\n",
    "H_text_matrix = nmf_text_model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37338425-2f1b-4a97-9e03-6950a3cf0893",
   "metadata": {},
   "source": [
    "## Done! Let’s look at our 10 topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "336421df-d65b-4c5a-a55e-a756c78393ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      " co (0.79)\n",
      " operation (0.65)\n",
      " disarmament (0.36)\n",
      " nuclear (0.34)\n",
      " relations (0.25)\n",
      "\n",
      "Topic 01\n",
      " terrorism (0.38)\n",
      " challenges (0.32)\n",
      " sustainable (0.30)\n",
      " millennium (0.29)\n",
      " reform (0.28)\n",
      "\n",
      "Topic 02\n",
      " africa (1.15)\n",
      " african (0.82)\n",
      " south (0.63)\n",
      " namibia (0.36)\n",
      " delegation (0.30)\n",
      "\n",
      "Topic 03\n",
      " arab (1.02)\n",
      " israel (0.89)\n",
      " palestinian (0.60)\n",
      " lebanon (0.54)\n",
      " israeli (0.54)\n",
      "\n",
      "Topic 04\n",
      " american (0.33)\n",
      " america (0.31)\n",
      " latin (0.31)\n",
      " panama (0.21)\n",
      " bolivia (0.21)\n",
      "\n",
      "Topic 05\n",
      " pacific (1.55)\n",
      " islands (1.23)\n",
      " solomon (0.86)\n",
      " island (0.82)\n",
      " fiji (0.71)\n",
      "\n",
      "Topic 06\n",
      " soviet (0.81)\n",
      " republic (0.78)\n",
      " nuclear (0.68)\n",
      " viet (0.64)\n",
      " socialist (0.63)\n",
      "\n",
      "Topic 07\n",
      " guinea (4.26)\n",
      " equatorial (1.75)\n",
      " bissau (1.53)\n",
      " papua (1.47)\n",
      " republic (0.57)\n",
      "\n",
      "Topic 08\n",
      " european (0.61)\n",
      " europe (0.44)\n",
      " cooperation (0.39)\n",
      " bosnia (0.34)\n",
      " herzegovina (0.30)\n",
      "\n",
      "Topic 09\n",
      " caribbean (0.98)\n",
      " small (0.66)\n",
      " bahamas (0.63)\n",
      " saint (0.63)\n",
      " barbados (0.61)\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, features, no_top_words=5):\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        total = words.sum()\n",
    "        largest = words.argsort()[::-1]  # invert sort order\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for i in range(0, no_top_words):\n",
    "            print(\" %s (%2.2f)\" % (features[largest[i]],\n",
    "                                   abs(words[largest[i]] * 100.0 / total)))\n",
    "\n",
    "# Call the function outside its definition with the appropriate arguments\n",
    "display_topics(nmf_text_model, tfidf_text_vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fdd0e5-7c95-4355-9278-96346c08a5f4",
   "metadata": {},
   "source": [
    "## Run same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1e527e7a-8612-40e4-81a9-4353cb971c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      " international (2.01)\n",
      " world (1.50)\n",
      " community (0.91)\n",
      " new (0.76)\n",
      " peace (0.67)\n",
      "\n",
      "Topic 01\n",
      " general (2.88)\n",
      " session (2.85)\n",
      " assembly (2.83)\n",
      " mr (2.00)\n",
      " president (1.83)\n",
      "\n",
      "Topic 02\n",
      " countries (4.41)\n",
      " developing (2.49)\n",
      " economic (1.51)\n",
      " developed (1.35)\n",
      " trade (0.92)\n",
      "\n",
      "Topic 03\n",
      " nations (5.61)\n",
      " united (5.50)\n",
      " organization (1.27)\n",
      " states (1.03)\n",
      " charter (0.93)\n",
      "\n",
      "Topic 04\n",
      " nuclear (4.91)\n",
      " weapons (3.25)\n",
      " disarmament (2.01)\n",
      " treaty (1.70)\n",
      " proliferation (1.45)\n",
      "\n",
      "Topic 05\n",
      " rights (6.51)\n",
      " human (6.20)\n",
      " respect (1.16)\n",
      " fundamental (0.86)\n",
      " universal (0.83)\n",
      "\n",
      "Topic 06\n",
      " africa (3.80)\n",
      " south (3.30)\n",
      " african (1.70)\n",
      " namibia (1.38)\n",
      " apartheid (1.18)\n",
      "\n",
      "Topic 07\n",
      " security (6.08)\n",
      " council (5.82)\n",
      " permanent (1.48)\n",
      " reform (1.47)\n",
      " peace (1.31)\n",
      "\n",
      "Topic 08\n",
      " people (1.35)\n",
      " peace (1.32)\n",
      " east (1.28)\n",
      " middle (1.17)\n",
      " palestinian (1.15)\n",
      "\n",
      "Topic 09\n",
      " development (4.51)\n",
      " sustainable (1.20)\n",
      " economic (1.07)\n",
      " social (1.00)\n",
      " goals (0.94)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Initialize NMF model\n",
    "nmf_para_model = NMF(n_components=10, random_state=42)\n",
    "\n",
    "# Fit NMF model to the TF-IDF vectors\n",
    "W_para_matrix = nmf_para_model.fit_transform(tfidf_para_vectors)\n",
    "H_para_matrix = nmf_para_model.components_\n",
    "\n",
    "# Display topics\n",
    "display_topics(nmf_para_model, tfidf_para_vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b329ad9-19fc-4916-9cf4-0711231d82d5",
   "metadata": {},
   "source": [
    " ## How “BIG” is each topic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "887f5c30-f684-4bec-929a-ced431e1c816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.06189432, 17.0213082 , 13.6565622 , 10.18695452, 11.35821532,\n",
       "        5.95003141,  7.90001101,  4.13937503, 11.91132462,  6.81432338])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_text_matrix.sum(axis=0)/W_text_matrix.sum()*100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80431fb9-5009-4ee2-99a9-804b8e8f057f",
   "metadata": {},
   "source": [
    "## Topic sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fbb3dd9e-0c95-4746-9b7d-2dc87cb0ade2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.99461472, 10.12384143, 10.13359996, 14.58547101,  6.74889346,\n",
       "        7.06521386,  8.61823051,  8.23846492, 11.85285437, 10.63881575])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_para_matrix.sum(axis=0)/W_para_matrix.sum()*100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874cac43-e46b-489f-b913-b24d229bce8e",
   "metadata": {},
   "source": [
    "## RUN this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffeb21e-6ed5-495f-983f-4d76a32d1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Convert the set of stopwords to a list\n",
    "stopwords_list = list(stopwords)\n",
    "\n",
    "# Assuming 'paragraph_df' is defined earlier\n",
    "count_para_vectorizer = CountVectorizer(stop_words=stopwords_list, min_df=5, max_df=0.7)\n",
    "count_para_vectors = count_para_vectorizer.fit_transform(paragraph_df[\"text\"])\n",
    "\n",
    "# Check the shape of the document-term matrix\n",
    "print(\"Shape of document-term matrix:\", count_para_vectors.shape)\n",
    "\n",
    "# Define and fit the LDA model\n",
    "lda_para_model = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "W_lda_para_matrix = lda_para_model.fit_transform(count_para_vectors)\n",
    "H_lda_para_matrix = lda_para_model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eebc547-123f-492a-8d4c-818f0cbb158c",
   "metadata": {},
   "source": [
    "## Start by cutting the paragraphs into words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8785ce52-85fc-42af-94aa-8bd6c4afcf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(279076, 25162)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Convert the stop words set to a list\n",
    "stopwords_list = list(stopwords)\n",
    "\n",
    "# Initialize CountVectorizer with the list of stop words\n",
    "count_para_vectorizer = CountVectorizer(stop_words=stopwords_list, min_df=5, max_df=0.7)\n",
    "\n",
    "# Fit and transform the text data\n",
    "count_para_vectors = count_para_vectorizer.fit_transform(paragraph_df[\"text\"])\n",
    "print(count_para_vectors.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0380b3b4-e333-4a2e-bb99-af977e3ee43c",
   "metadata": {},
   "source": [
    "## Run the LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e07cacbf-16b7-4e6f-8f57-34f2877c580d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 1:\n",
      "general assembly session president mr\n",
      "\n",
      "Topic 2:\n",
      "world human people peace nations\n",
      "\n",
      "Topic 3:\n",
      "nuclear weapons disarmament arms states\n",
      "\n",
      "Topic 4:\n",
      "international security united nations council\n",
      "\n",
      "Topic 5:\n",
      "countries economic developing development international\n",
      "\n",
      "Topic 6:\n",
      "peace east middle people israel\n",
      "\n",
      "Topic 7:\n",
      "states countries peace relations peoples\n",
      "\n",
      "Topic 8:\n",
      "africa south united nations african\n",
      "\n",
      "Topic 9:\n",
      "people republic government united nations\n",
      "\n",
      "Topic 10:\n",
      "nations united development international world\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words=5):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic %d:\" % (topic_idx + 1))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "# Get feature names from the vocabulary generated by CountVectorizer\n",
    "feature_names = count_para_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display topics using the vocabulary generated by CountVectorizer\n",
    "display_topics(lda_para_model, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9354db-cf97-48a8-8e1c-6a9539ee1d63",
   "metadata": {},
   "source": [
    "## Topic sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "72648423-f5c7-4b11-af73-a59e551037a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.96413708, 13.1939403 ,  7.04455321, 11.08311812,  9.84055263,\n",
       "        8.80334442,  8.89408336,  6.94897733,  9.48290182, 15.74439172])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_lda_para_matrix.sum(axis=0)/W_lda_para_matrix.sum()*100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa5cadf-1555-4bc6-a43d-a7a9361f09b0",
   "metadata": {},
   "source": [
    "## draw wordclouds for the 10 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fe19fba1-05a3-4876-807a-11b3c5b76037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.output_scroll { height: 44em; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>div.output_scroll { height: 44em; }</style>\"))\n",
    "\n",
    "def wordcloud_topics(model, features, no_top_words=40):\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        size = {}\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        for i in range(0, no_top_words):\n",
    "            size[features[largest[i]]] = abs(words[largest[i]])\n",
    "        wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
    "        wc.generate_from_frequencies(size)\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        # if you don't want to save the topic model, comment the next line\n",
    "        plt.savefig(f'topic{topic}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7df2b0-931d-480d-88b0-0cb3eaad795a",
   "metadata": {},
   "source": [
    "## The NMF wordclouds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "db6c7df2-af0d-4b2c-8288-0518637391fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TfidfVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m wordcloud_topics(nmf_para_model, tfidf_para_vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "wordcloud_topics(nmf_para_model, tfidf_para_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d252774a-95ce-41a0-9a07-fd98284879f0",
   "metadata": {},
   "source": [
    "## The LDA wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08df6ef-ba79-4f1c-8db8-468899b64dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_topics(lda_para_model, count_para_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12eb684-2be2-49c5-950c-16043f001934",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = []\n",
    "voc = tfidf_para_vectorizer.get_feature_names()\n",
    "for topic in nmf_para_model.components_:\n",
    "important = topic.argsort()\n",
    "top_word = voc[important[-1]] + \" \" + voc[important[-2]]\n",
    "topic_names.append(\"Topic \" + top_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e31633-2c87-4d4d-9776-452961c689f0",
   "metadata": {},
   "source": [
    "## Separate data by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879fee9c-7ff3-462e-9abd-ec53c1fddbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "year_data = []\n",
    "for year in tqdm(np.unique(np.unique(paragraph_df[\"year\"]))):\n",
    "W_year = nmf_para_model.transform(tfidf_para_vectors[np.array(paragraph_\n",
    "df[\"year\"] == year)])\n",
    "year_data.append([year] +\n",
    "list(W_year.sum(axis=0)/W_year.sum()*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580c4d27-feb9-422e-a5fa-05127170d61a",
   "metadata": {},
   "source": [
    "## Draw time chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60abd63c-b483-4fe1-b659-01a54e0ea7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df_year = pd.DataFrame(year_data, columns=[\"year\"] + topic_names).set_index(\"year\")\n",
    "df_year.plot.area(figsize=(16,9))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
